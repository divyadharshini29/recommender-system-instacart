{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System for Instacart\n",
    "\n",
    "### Neighborhood-based model using Term Frequency-Inverse Document Frequency (TF-IDF) and Cosine Similarity\n",
    "\n",
    "In this notebook, a neighborhood-based model is implemented to generate recommendations for a target user, by using cosine similarities on a TF-IDF weighted utility matrix.\n",
    "\n",
    "We are using the library [`implicit`](https://github.com/benfred/implicit) to calculates tfidf_weight, [`scipy`](https://www.scipy.org) to implement sparse matrix, and [`sklearn`](http://scikit-learn.org/stable/) to calculate cosine similarity.\n",
    "\n",
    "The following steps are conducted: \n",
    "- Utlity Matrix is generated, where every entry represents a Term Frequency(TF).\n",
    "- Based on the utility matrix, a TF-IDF weight matrix is calculated.\n",
    "- Given a target user (a row in the TF-IDF matrix), the cosine similarity vector of the target user is produced.\n",
    "- Select top K similar users and use PriorityQueue to select top N products based on overall sales in the candidate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "from implicit.nearest_neighbours import tfidf_weight\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from numpy import bincount, log, sqrt\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper Functions\n",
    "\n",
    "def sparsity(matrix):\n",
    "    \"\"\"\n",
    "    Given a matrix, returns its sparsity\n",
    "    \"\"\"\n",
    "    total_size = matrix.shape[0] * matrix.shape[1]\n",
    "    actual_size = matrix.size\n",
    "    sparsity = (1 - (actual_size / total_size)) * 100\n",
    "    return(sparsity)\n",
    "\n",
    "\n",
    "def get_k_popular(k, df_merged_order_products_prior):\n",
    "    \"\"\"\n",
    "    Returns the `k` most popular products based on purchase count in the dataset\n",
    "    \"\"\"\n",
    "    popular_products = list(df_merged_order_products_prior[\"product_id\"].value_counts().head(k).index)\n",
    "    return popular_products\n",
    "\n",
    "\n",
    "def make_prior_data():\n",
    "    \"\"\"\n",
    "    Generates the prior dataset including prior_user_products and product_frequency\n",
    "    \"\"\"\n",
    "    # Read prior order csv\n",
    "    df_order_products_prior = pd.read_csv(\"../data/order_products__prior.csv\")\n",
    "    current_order_user_df = df_orders.loc[(df_orders.eval_set == \"prior\")].reset_index()\n",
    "    current_order_user_df = current_order_user_df[[\"order_id\", \"user_id\"]]\n",
    "\n",
    "    assert len(current_order_user_df[\"order_id\"].unique()) == len(df_order_products_prior[\"order_id\"].unique())\n",
    "\n",
    "    # Group product_id for each order into products\n",
    "    df_order_products_prior = df_order_products_prior[[\"order_id\", \"product_id\"]]\n",
    "    df_product_frequency = df_order_products_prior['product_id'].value_counts()\n",
    "    df_order_products_prior = df_order_products_prior.groupby(\"order_id\")[\"product_id\"].apply(list).reset_index().rename(columns={\"product_id\": \"products\"})\n",
    "    \n",
    "    \n",
    "    assert current_order_user_df.size == df_order_products_prior.size\n",
    "\n",
    "    df_prior_user_products = pd.merge(current_order_user_df, df_order_products_prior, on=\"order_id\")\n",
    "    df_prior_user_products = df_prior_user_products[[\"user_id\", \"products\"]]\n",
    "    df_prior_user_products = df_prior_user_products.groupby(\"user_id\")[\"products\"].agg(sum).reset_index()\n",
    "\n",
    "    return df_prior_user_products, df_product_frequency\n",
    "\n",
    "\n",
    "def make_test_data(test_data_path, df_orders, df_order_products_train):\n",
    "    \"\"\"\n",
    "    Generates the test dataset and saves it to disk at the given path\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Creating test data ...\")\n",
    "\n",
    "    # Read train csv\n",
    "    df_order_user_current = df_orders.loc[(df_orders.eval_set == \"train\")].reset_index()\n",
    "    df_order_user_current = df_order_user_current[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Sanity check #1: `current_order_user_df` and `df_order_products_train` should have the same number of \n",
    "    # unique order ids\n",
    "    assert len(df_order_user_current[\"order_id\"].unique()) == len(df_order_products_train[\"order_id\"].unique())\n",
    "\n",
    "    # Convert train dataframe to a similar format\n",
    "    df_order_products_test = df_order_products_train[[\"order_id\", \"product_id\"]]\n",
    "    df_order_products_test = df_order_products_test.groupby(\"order_id\")[\"product_id\"].apply(list).reset_index().rename(columns={\"product_id\": \"products\"})\n",
    "\n",
    "    # Sanity check #2: `df_order_products_test` and `df_order_user_current` should have the same number of \n",
    "    # records before attempting to merge them\n",
    "    assert df_order_products_test.size == df_order_user_current.size\n",
    "\n",
    "    # Merge on order id\n",
    "    df_user_products_test = pd.merge(df_order_user_current, df_order_products_test, on=\"order_id\")\n",
    "    df_user_products_test = df_user_products_test[[\"user_id\", \"products\"]]\n",
    "\n",
    "    # Write to disk\n",
    "    df_user_products_test.to_csv(test_data_path, index_label=False)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "def save_data_to_disk(dataframe, df_name):\n",
    "    \"\"\"\n",
    "    Save the data to disk\n",
    "    \"\"\"\n",
    "    filepath = \"../data/df_{}.pkl\".format(df_name)\n",
    "    dataframe.to_pickle(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order datasets\n",
    "df_order_products_prior = pd.read_csv(\"../data/order_products__prior.csv\")\n",
    "df_order_products_train = pd.read_csv(\"../data/order_products__train.csv\")\n",
    "df_orders = pd.read_csv(\"../data/orders.csv\") \n",
    "\n",
    "# Products\n",
    "df_products = pd.read_csv(\"../data/products.csv\")\n",
    "\n",
    "# Make prior data\n",
    "# Running time: 3 min\n",
    "df_prior_user_products, df_product_frequency = make_prior_data()\n",
    "\n",
    "# Merge prior orders and products\n",
    "df_merged_order_products_prior = pd.merge(df_order_products_prior, df_products, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you'd like to save time, you should save data to disk, running time : 2 min\n",
    "save_data_to_disk(df_prior_user_products, \"user_products\")\n",
    "save_data_to_disk(df_product_frequency, \"product_frequency\")\n",
    "\n",
    "# Read user_products and product_frequency from the disk\n",
    "df_prior_user_products = pd.read_pickle(\"../data/df_user_products.pkl\")\n",
    "df_product_frequency = pd.read_pickle(\"../data/df_product_frequency.pkl\")\n",
    "df_product_frequency = pd.DataFrame(df_product_frequency).rename(columns={\"product_id\": \"frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make test data\n",
    "REBUILD_TEST_DATA = False\n",
    "test_data_path = \"../data/user_products__test.csv\"\n",
    "if REBUILD_TEST_DATA or not Path(test_data_path).is_file():\n",
    "    make_test_data(test_data_path, df_orders, df_order_products_train)\n",
    "df_user_products_test = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[196, 25133, 38928, 26405, 39657, 10258, 13032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[22963, 7963, 16589, 32792, 41787, 22825, 1364...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[15349, 19057, 16185, 21413, 20843, 20114, 482...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>[12053, 47272, 37999, 13198, 43967, 40852, 176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[15937, 5539, 10960, 23165, 22247, 4853, 27104...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                           products\n",
       "0        1  [196, 25133, 38928, 26405, 39657, 10258, 13032...\n",
       "1        2  [22963, 7963, 16589, 32792, 41787, 22825, 1364...\n",
       "2        5  [15349, 19057, 16185, 21413, 20843, 20114, 482...\n",
       "3        7  [12053, 47272, 37999, 13198, 43967, 40852, 176...\n",
       "4        8  [15937, 5539, 10960, 23165, 22247, 4853, 27104..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_products_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Product Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_product_prior_df(filepath, df_orders, df_order_products_prior):\n",
    "    \"\"\"\n",
    "    Generates a dataframe of users and their prior products purchases, and writes it to disk at the given path\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Creating prior user product data frame ...\")\n",
    "    \n",
    "    # Consider ony \"prior\" orders and remove all columns except `user_id` from `df_orders`\n",
    "    df_order_user_prior = df_orders.loc[df_orders.eval_set == \"prior\"]\n",
    "    df_order_user_prior = df_order_user_prior[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Remove all columns except order_id and user_id from df_orders and \n",
    "    # merge the above on `order_id` and remove `order_id`\n",
    "    df_merged = pd.merge(df_order_user_prior, df_order_products_prior[[\"order_id\", \"product_id\"]], on=\"order_id\")\n",
    "    df_user_product_prior = df_merged[[\"user_id\", \"product_id\"]]\n",
    "    df_user_product_prior = df_user_product_prior.groupby([\"user_id\", \"product_id\"]).size().reset_index().rename(columns={0:\"quantity\"})\n",
    "    \n",
    "    # Write to disk\n",
    "    df_user_product_prior.to_csv(filepath, index_label=False)\n",
    "\n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Build dataframe of users, products and quantity bought using prior datasets\n",
    "REBUILD_MATRIX_DF = False\n",
    "matrix_df_path = \"../data/user_products__prior.csv\"\n",
    "if REBUILD_MATRIX_DF or not Path(matrix_df_path).is_file():\n",
    "    get_user_product_prior_df(matrix_df_path, df_orders, df_order_products_prior)\n",
    "df_user_product_prior = pd.read_csv(matrix_df_path)\n",
    "df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_product_user_matrix(matrix_path, df_user_product_prior):\n",
    "    \"\"\"\n",
    "    Generates a utility matrix representing purchase history of users, and writes it to disk.\n",
    "    Rows and Columns represent products and users respectively.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Creating product user matrix ...\")\n",
    "    \n",
    "    # Make the dataframe a sparse matrix\n",
    "    df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "    df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")\n",
    "    product_user_matrix = sparse.coo_matrix((df_user_product_prior[\"quantity\"],\n",
    "                                            (df_user_product_prior[\"product_id\"].cat.codes.copy(),\n",
    "                                             df_user_product_prior[\"user_id\"].cat.codes.copy())))\n",
    "    \n",
    "    sparse.save_npz(matrix_path, product_user_matrix)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Get the `product x user` matrix\n",
    "REBUILD_MATRIX = False\n",
    "matrix_path = \"../data/product_user_matrix.npz\"\n",
    "if REBUILD_MATRIX or not Path(matrix_path).is_file():\n",
    "    build_product_user_matrix(matrix_path, df_user_product_prior)\n",
    "product_user_matrix = sparse.load_npz(matrix_path).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User=1 bought product=196 10 times\n",
    "assert product_user_matrix[195, 0] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.8700882953749"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity(product_user_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch Term Frequency matrix\n",
    "user_product_matrix = product_user_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_weight(tf):\n",
    "    \"\"\"\n",
    "    Given a Term Frequency matrix\n",
    "    Returns a TF-IDF weight matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    tf_idf = coo_matrix(tf)\n",
    "\n",
    "    # calculate IDF\n",
    "    N = float(tf_idf.shape[0])\n",
    "    idf = log(N / (1 + bincount(tf_idf.col)))\n",
    "\n",
    "    # apply TF-IDF adjustment\n",
    "    tf_idf.data = sqrt(tf_idf.data) * idf[tf_idf.col]\n",
    "    return tf_idf\n",
    "\n",
    "tf_idf = tfidf_weight(user_product_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to Compressed Sparse Row format\n",
    "tf_idf = tf_idf.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting one user to test\n",
    "target_user_id = 1\n",
    "\n",
    "# Fetch row of target user\n",
    "target_user = tf_idf[target_user_id - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity Vector of target user\n",
    "similarities = cosine_similarity(tf_idf, target_user, False)\n",
    "cos_vec = similarities.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateRecommendations(target_user, cos_vec, K, N):\n",
    "    \"\"\"\n",
    "    Given a target_user (a row), a cosine similarity vector, the number of similar users K, \n",
    "          the number of products to be recommended.\n",
    "    Returns product set by target user and N recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select top K similar users\n",
    "    top_K_similar_users = heapq.nlargest(K+1, range(len(cos_vec)), cos_vec.take)\n",
    "\n",
    "    # Initialize the result for recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # Exclude the user with same purchase history (1.00000) as the target user and implement set-minus\n",
    "    products_target_user = df_prior_user_products.loc[df_prior_user_products['user_id'] == target_user_id].products\n",
    "\n",
    "    # Products of Target User\n",
    "    productset_target_user = set(products_target_user.tolist()[0])\n",
    "\n",
    "    # Fetch the preliminary recommendations\n",
    "    for similar_user_id in top_K_similar_users:\n",
    "        \n",
    "        products_similar_user = df_prior_user_products.loc[df_prior_user_products['user_id'] == similar_user_id + 1].products\n",
    "\n",
    "        # Recommend the products bought by the user who firstly differs in the purchase history from A.\n",
    "        candidate_recommendation = set(products_similar_user.tolist()[0]) - productset_target_user\n",
    "\n",
    "        # If similar_user_id equals to target_user_id or the candidate_recommendation is empty,\n",
    "        # skip current user\n",
    "        if similar_user_id == target_user_id or not candidate_recommendation: continue\n",
    "\n",
    "        # One candidate_recommendation found, and extend it to the result\n",
    "        recommendations.extend(candidate_recommendation)\n",
    "\n",
    "        # If length of recommendations exceed N, break\n",
    "        # Needed because this will ensure the recommentations are the products bought by most similar users\n",
    "        if len(recommendations) > N: break\n",
    "        \n",
    "    # Pick the top N popularity (overall sales) to recommend\n",
    "    h = []\n",
    "    for rec in recommendations:\n",
    "        heapq.heappush(h, (df_product_frequency.loc[rec]['frequency'], rec))\n",
    "        if len(h) > N:\n",
    "            heapq.heappop(h)\n",
    "            \n",
    "    return productset_target_user, [item[1] for item in h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "productset_target_user, recommendations = generateRecommendations(target_user, similarities.toarray(), 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual products bought by User 1:\n",
      "{17122, 196, 26405, 14084, 46149, 26088, 13032, 39657, 12427, 25133, 35951, 38928, 10258, 30450, 49235, 10326, 13176, 41787}\n",
      "\n",
      "Recommended products for User 1:\n",
      "[42265, 39275, 39275, 16797, 16797, 16797, 16797, 16797, 21137, 21137]\n"
     ]
    }
   ],
   "source": [
    "# Output the product_name of Target User's products as well as Recommendations\n",
    "print('Actual products bought by User {}:'.format(target_user_id))\n",
    "print(productset_target_user)\n",
    "print()\n",
    "print('Recommended products for User {}:'.format(target_user_id))\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the 10 most popular products\n",
    "popular_products = get_k_popular(10, df_merged_order_products_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataframe with recall values ...\n",
      "0.15% completed\n",
      "0.30% completed\n",
      "0.46% completed\n",
      "0.61% completed\n",
      "0.76% completed\n",
      "0.91% completed\n",
      "1.07% completed\n",
      "1.22% completed\n",
      "1.37% completed\n",
      "1.52% completed\n",
      "1.68% completed\n",
      "1.83% completed\n",
      "1.98% completed\n",
      "2.13% completed\n",
      "2.29% completed\n",
      "2.44% completed\n",
      "2.59% completed\n",
      "2.74% completed\n",
      "2.90% completed\n",
      "3.05% completed\n",
      "3.20% completed\n",
      "3.35% completed\n",
      "3.51% completed\n",
      "3.66% completed\n",
      "3.81% completed\n",
      "3.96% completed\n",
      "4.12% completed\n",
      "4.27% completed\n",
      "4.42% completed\n",
      "4.57% completed\n",
      "4.73% completed\n",
      "4.88% completed\n",
      "5.03% completed\n",
      "5.18% completed\n",
      "5.33% completed\n",
      "5.49% completed\n",
      "5.64% completed\n",
      "5.79% completed\n",
      "5.94% completed\n",
      "6.10% completed\n",
      "6.25% completed\n",
      "6.40% completed\n",
      "6.55% completed\n",
      "6.71% completed\n",
      "6.86% completed\n",
      "7.01% completed\n",
      "7.16% completed\n",
      "7.32% completed\n",
      "7.47% completed\n",
      "7.62% completed\n",
      "7.77% completed\n",
      "7.93% completed\n",
      "8.08% completed\n",
      "8.23% completed\n",
      "8.38% completed\n",
      "8.54% completed\n",
      "8.69% completed\n",
      "8.84% completed\n",
      "8.99% completed\n",
      "9.15% completed\n",
      "9.30% completed\n",
      "9.45% completed\n",
      "9.60% completed\n",
      "9.76% completed\n",
      "9.91% completed\n",
      "10.06% completed\n",
      "10.21% completed\n",
      "10.37% completed\n",
      "10.52% completed\n",
      "10.67% completed\n",
      "10.82% completed\n",
      "10.97% completed\n",
      "11.13% completed\n",
      "11.28% completed\n",
      "11.43% completed\n",
      "11.58% completed\n",
      "11.74% completed\n",
      "11.89% completed\n",
      "12.04% completed\n",
      "12.19% completed\n",
      "12.35% completed\n",
      "12.50% completed\n",
      "12.65% completed\n",
      "12.80% completed\n",
      "12.96% completed\n",
      "13.11% completed\n",
      "13.26% completed\n",
      "13.41% completed\n",
      "13.57% completed\n",
      "13.72% completed\n",
      "13.87% completed\n",
      "14.02% completed\n",
      "14.18% completed\n",
      "14.33% completed\n",
      "14.48% completed\n",
      "14.63% completed\n",
      "14.79% completed\n",
      "14.94% completed\n",
      "15.09% completed\n",
      "15.24% completed\n",
      "15.40% completed\n",
      "15.55% completed\n",
      "15.70% completed\n",
      "15.85% completed\n",
      "16.00% completed\n",
      "16.16% completed\n",
      "16.31% completed\n",
      "16.46% completed\n",
      "16.61% completed\n",
      "16.77% completed\n",
      "16.92% completed\n",
      "17.07% completed\n",
      "17.22% completed\n",
      "17.38% completed\n",
      "17.53% completed\n",
      "17.68% completed\n",
      "17.83% completed\n",
      "17.99% completed\n",
      "18.14% completed\n",
      "18.29% completed\n",
      "18.44% completed\n",
      "18.60% completed\n",
      "18.75% completed\n",
      "18.90% completed\n",
      "19.05% completed\n",
      "19.21% completed\n",
      "19.36% completed\n",
      "19.51% completed\n",
      "19.66% completed\n",
      "19.82% completed\n",
      "19.97% completed\n",
      "20.12% completed\n",
      "20.27% completed\n",
      "20.43% completed\n",
      "20.58% completed\n",
      "20.73% completed\n",
      "20.88% completed\n",
      "21.04% completed\n",
      "21.19% completed\n",
      "21.34% completed\n",
      "21.49% completed\n",
      "21.64% completed\n",
      "21.80% completed\n",
      "21.95% completed\n",
      "22.10% completed\n",
      "22.25% completed\n",
      "22.41% completed\n",
      "22.56% completed\n",
      "22.71% completed\n",
      "22.86% completed\n",
      "23.02% completed\n",
      "23.17% completed\n",
      "23.32% completed\n",
      "23.47% completed\n",
      "23.63% completed\n",
      "23.78% completed\n",
      "23.93% completed\n",
      "24.08% completed\n",
      "24.24% completed\n",
      "24.39% completed\n",
      "24.54% completed\n",
      "24.69% completed\n",
      "24.85% completed\n",
      "25.00% completed\n",
      "25.15% completed\n",
      "25.30% completed\n",
      "25.46% completed\n",
      "25.61% completed\n",
      "25.76% completed\n",
      "25.91% completed\n",
      "26.07% completed\n",
      "26.22% completed\n",
      "26.37% completed\n",
      "26.52% completed\n",
      "26.67% completed\n",
      "26.83% completed\n",
      "26.98% completed\n",
      "27.13% completed\n",
      "27.28% completed\n",
      "27.44% completed\n",
      "27.59% completed\n",
      "27.74% completed\n",
      "27.89% completed\n",
      "28.05% completed\n",
      "28.20% completed\n",
      "28.35% completed\n",
      "28.50% completed\n",
      "28.66% completed\n",
      "28.81% completed\n",
      "28.96% completed\n",
      "29.11% completed\n",
      "29.27% completed\n",
      "29.42% completed\n",
      "29.57% completed\n",
      "29.72% completed\n",
      "29.88% completed\n",
      "30.03% completed\n",
      "30.18% completed\n",
      "30.33% completed\n",
      "30.49% completed\n",
      "30.64% completed\n",
      "30.79% completed\n",
      "30.94% completed\n",
      "31.10% completed\n",
      "31.25% completed\n",
      "31.40% completed\n",
      "31.55% completed\n",
      "31.71% completed\n",
      "31.86% completed\n",
      "32.01% completed\n",
      "32.16% completed\n",
      "32.31% completed\n",
      "32.47% completed\n",
      "32.62% completed\n",
      "32.77% completed\n",
      "32.92% completed\n",
      "33.08% completed\n",
      "33.23% completed\n",
      "33.38% completed\n",
      "33.53% completed\n",
      "33.69% completed\n",
      "33.84% completed\n",
      "33.99% completed\n",
      "34.14% completed\n",
      "34.30% completed\n",
      "34.45% completed\n",
      "34.60% completed\n",
      "34.75% completed\n",
      "34.91% completed\n",
      "35.06% completed\n",
      "35.21% completed\n",
      "35.36% completed\n",
      "35.52% completed\n",
      "35.67% completed\n",
      "35.82% completed\n",
      "35.97% completed\n",
      "36.13% completed\n",
      "36.28% completed\n",
      "36.43% completed\n",
      "36.58% completed\n",
      "36.74% completed\n",
      "36.89% completed\n",
      "37.04% completed\n",
      "37.19% completed\n",
      "37.34% completed\n",
      "37.50% completed\n",
      "37.65% completed\n",
      "37.80% completed\n",
      "37.95% completed\n",
      "38.11% completed\n",
      "38.26% completed\n",
      "38.41% completed\n",
      "38.56% completed\n",
      "38.72% completed\n",
      "38.87% completed\n",
      "39.02% completed\n",
      "39.17% completed\n",
      "39.33% completed\n",
      "39.48% completed\n",
      "39.63% completed\n",
      "39.78% completed\n",
      "39.94% completed\n",
      "40.09% completed\n",
      "40.24% completed\n",
      "40.39% completed\n",
      "40.55% completed\n",
      "40.70% completed\n",
      "40.85% completed\n",
      "41.00% completed\n",
      "41.16% completed\n",
      "41.31% completed\n",
      "41.46% completed\n",
      "41.61% completed\n",
      "41.77% completed\n",
      "41.92% completed\n",
      "42.07% completed\n",
      "42.22% completed\n",
      "42.38% completed\n",
      "42.53% completed\n",
      "42.68% completed\n",
      "42.83% completed\n",
      "42.98% completed\n",
      "43.14% completed\n",
      "43.29% completed\n",
      "43.44% completed\n",
      "43.59% completed\n",
      "43.75% completed\n",
      "43.90% completed\n",
      "44.05% completed\n",
      "44.20% completed\n",
      "44.36% completed\n",
      "44.51% completed\n",
      "44.66% completed\n",
      "44.81% completed\n",
      "44.97% completed\n",
      "45.12% completed\n",
      "45.27% completed\n",
      "45.42% completed\n",
      "45.58% completed\n",
      "45.73% completed\n",
      "45.88% completed\n",
      "46.03% completed\n",
      "46.19% completed\n",
      "46.34% completed\n",
      "46.49% completed\n",
      "46.64% completed\n",
      "46.80% completed\n",
      "46.95% completed\n",
      "47.10% completed\n",
      "47.25% completed\n",
      "47.41% completed\n",
      "47.56% completed\n",
      "47.71% completed\n",
      "47.86% completed\n",
      "48.01% completed\n",
      "48.17% completed\n",
      "48.32% completed\n",
      "48.47% completed\n",
      "48.62% completed\n",
      "48.78% completed\n",
      "48.93% completed\n",
      "49.08% completed\n",
      "49.23% completed\n",
      "49.39% completed\n",
      "49.54% completed\n",
      "49.69% completed\n",
      "49.84% completed\n",
      "50.00% completed\n",
      "50.15% completed\n",
      "50.30% completed\n",
      "50.45% completed\n",
      "50.61% completed\n",
      "50.76% completed\n",
      "50.91% completed\n",
      "51.06% completed\n",
      "51.22% completed\n",
      "51.37% completed\n",
      "51.52% completed\n",
      "51.67% completed\n",
      "51.83% completed\n",
      "51.98% completed\n",
      "52.13% completed\n",
      "52.28% completed\n",
      "52.44% completed\n",
      "52.59% completed\n",
      "52.74% completed\n",
      "52.89% completed\n",
      "53.05% completed\n",
      "53.20% completed\n",
      "53.35% completed\n",
      "53.50% completed\n",
      "53.65% completed\n",
      "53.81% completed\n",
      "53.96% completed\n",
      "54.11% completed\n",
      "54.26% completed\n",
      "54.42% completed\n",
      "54.57% completed\n",
      "54.72% completed\n",
      "54.87% completed\n",
      "55.03% completed\n",
      "55.18% completed\n",
      "55.33% completed\n",
      "55.48% completed\n",
      "55.64% completed\n",
      "55.79% completed\n",
      "55.94% completed\n",
      "56.09% completed\n",
      "56.25% completed\n",
      "56.40% completed\n",
      "56.55% completed\n",
      "56.70% completed\n",
      "56.86% completed\n",
      "57.01% completed\n",
      "57.16% completed\n",
      "57.31% completed\n",
      "57.47% completed\n",
      "57.62% completed\n",
      "57.77% completed\n",
      "57.92% completed\n",
      "58.08% completed\n",
      "58.23% completed\n",
      "58.38% completed\n",
      "58.53% completed\n",
      "58.68% completed\n",
      "58.84% completed\n",
      "58.99% completed\n",
      "59.14% completed\n",
      "59.29% completed\n",
      "59.45% completed\n",
      "59.60% completed\n",
      "59.75% completed\n",
      "59.90% completed\n",
      "60.06% completed\n",
      "60.21% completed\n",
      "60.36% completed\n",
      "60.51% completed\n",
      "60.67% completed\n",
      "60.82% completed\n",
      "60.97% completed\n",
      "61.12% completed\n",
      "61.28% completed\n",
      "61.43% completed\n",
      "61.58% completed\n",
      "61.73% completed\n",
      "61.89% completed\n",
      "62.04% completed\n",
      "62.19% completed\n",
      "62.34% completed\n",
      "62.50% completed\n",
      "62.65% completed\n",
      "62.80% completed\n",
      "62.95% completed\n",
      "63.11% completed\n",
      "63.26% completed\n",
      "63.41% completed\n",
      "63.56% completed\n",
      "63.72% completed\n",
      "63.87% completed\n",
      "64.02% completed\n",
      "64.17% completed\n",
      "64.32% completed\n",
      "64.48% completed\n",
      "64.63% completed\n",
      "64.78% completed\n",
      "64.93% completed\n",
      "65.09% completed\n",
      "65.24% completed\n",
      "65.39% completed\n",
      "65.54% completed\n",
      "65.70% completed\n",
      "65.85% completed\n",
      "66.00% completed\n",
      "66.15% completed\n",
      "66.31% completed\n",
      "66.46% completed\n",
      "66.61% completed\n",
      "66.76% completed\n",
      "66.92% completed\n",
      "67.07% completed\n",
      "67.22% completed\n",
      "67.37% completed\n",
      "67.53% completed\n",
      "67.68% completed\n",
      "67.83% completed\n",
      "67.98% completed\n",
      "68.14% completed\n",
      "68.29% completed\n",
      "68.44% completed\n",
      "68.59% completed\n",
      "68.75% completed\n",
      "68.90% completed\n",
      "69.05% completed\n",
      "69.20% completed\n",
      "69.35% completed\n",
      "69.51% completed\n",
      "69.66% completed\n",
      "69.81% completed\n",
      "69.96% completed\n",
      "70.12% completed\n",
      "70.27% completed\n",
      "70.42% completed\n",
      "70.57% completed\n",
      "70.73% completed\n",
      "70.88% completed\n",
      "71.03% completed\n",
      "71.18% completed\n",
      "71.34% completed\n",
      "71.49% completed\n",
      "71.64% completed\n",
      "71.79% completed\n",
      "71.95% completed\n",
      "72.10% completed\n",
      "72.25% completed\n",
      "72.40% completed\n",
      "72.56% completed\n",
      "72.71% completed\n",
      "72.86% completed\n",
      "73.01% completed\n",
      "73.17% completed\n",
      "73.32% completed\n",
      "73.47% completed\n",
      "73.62% completed\n",
      "73.78% completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.93% completed\n",
      "74.08% completed\n",
      "74.23% completed\n",
      "74.39% completed\n",
      "74.54% completed\n",
      "74.69% completed\n",
      "74.84% completed\n",
      "74.99% completed\n",
      "75.15% completed\n",
      "75.30% completed\n",
      "75.45% completed\n",
      "75.60% completed\n",
      "75.76% completed\n",
      "75.91% completed\n",
      "76.06% completed\n",
      "76.21% completed\n",
      "76.37% completed\n",
      "76.52% completed\n",
      "76.67% completed\n",
      "76.82% completed\n",
      "76.98% completed\n",
      "77.13% completed\n",
      "77.28% completed\n",
      "77.43% completed\n",
      "77.59% completed\n",
      "77.74% completed\n",
      "77.89% completed\n",
      "78.04% completed\n",
      "78.20% completed\n",
      "78.35% completed\n",
      "78.50% completed\n",
      "78.65% completed\n",
      "78.81% completed\n",
      "78.96% completed\n",
      "79.11% completed\n",
      "79.26% completed\n",
      "79.42% completed\n",
      "79.57% completed\n",
      "79.72% completed\n",
      "79.87% completed\n",
      "80.02% completed\n",
      "80.18% completed\n",
      "80.33% completed\n",
      "80.48% completed\n",
      "80.63% completed\n",
      "80.79% completed\n",
      "80.94% completed\n",
      "81.09% completed\n",
      "81.24% completed\n",
      "81.40% completed\n",
      "81.55% completed\n",
      "81.70% completed\n",
      "81.85% completed\n",
      "82.01% completed\n",
      "82.16% completed\n",
      "82.31% completed\n",
      "82.46% completed\n",
      "82.62% completed\n",
      "82.77% completed\n",
      "82.92% completed\n",
      "83.07% completed\n",
      "83.23% completed\n",
      "83.38% completed\n",
      "83.53% completed\n",
      "83.68% completed\n",
      "83.84% completed\n",
      "83.99% completed\n",
      "84.14% completed\n",
      "84.29% completed\n",
      "84.45% completed\n",
      "84.60% completed\n",
      "84.75% completed\n",
      "84.90% completed\n",
      "85.06% completed\n",
      "85.21% completed\n",
      "85.36% completed\n",
      "85.51% completed\n",
      "85.66% completed\n",
      "85.82% completed\n",
      "85.97% completed\n",
      "86.12% completed\n",
      "86.27% completed\n",
      "86.43% completed\n",
      "86.58% completed\n",
      "86.73% completed\n",
      "86.88% completed\n",
      "87.04% completed\n",
      "87.19% completed\n",
      "87.34% completed\n",
      "87.49% completed\n",
      "87.65% completed\n",
      "87.80% completed\n",
      "87.95% completed\n",
      "88.10% completed\n",
      "88.26% completed\n",
      "88.41% completed\n",
      "88.56% completed\n",
      "88.71% completed\n",
      "88.87% completed\n",
      "89.02% completed\n",
      "89.17% completed\n",
      "89.32% completed\n",
      "89.48% completed\n",
      "89.63% completed\n",
      "89.78% completed\n",
      "89.93% completed\n",
      "90.09% completed\n",
      "90.24% completed\n",
      "90.39% completed\n",
      "90.54% completed\n",
      "90.69% completed\n",
      "90.85% completed\n",
      "91.00% completed\n",
      "91.15% completed\n",
      "91.30% completed\n",
      "91.46% completed\n",
      "91.61% completed\n",
      "91.76% completed\n",
      "91.91% completed\n",
      "92.07% completed\n",
      "92.22% completed\n",
      "92.37% completed\n",
      "92.52% completed\n",
      "92.68% completed\n",
      "92.83% completed\n",
      "92.98% completed\n",
      "93.13% completed\n",
      "93.29% completed\n",
      "93.44% completed\n",
      "93.59% completed\n",
      "93.74% completed\n",
      "93.90% completed\n",
      "94.05% completed\n",
      "94.20% completed\n",
      "94.35% completed\n",
      "94.51% completed\n",
      "94.66% completed\n",
      "94.81% completed\n",
      "94.96% completed\n",
      "95.12% completed\n",
      "95.27% completed\n",
      "95.42% completed\n",
      "95.57% completed\n",
      "95.73% completed\n",
      "95.88% completed\n",
      "96.03% completed\n",
      "96.18% completed\n",
      "96.33% completed\n",
      "96.49% completed\n",
      "96.64% completed\n",
      "96.79% completed\n",
      "96.94% completed\n",
      "97.10% completed\n",
      "97.25% completed\n",
      "97.40% completed\n",
      "97.55% completed\n",
      "97.71% completed\n",
      "97.86% completed\n",
      "98.01% completed\n",
      "98.16% completed\n",
      "98.32% completed\n",
      "98.47% completed\n",
      "98.62% completed\n",
      "98.77% completed\n",
      "98.93% completed\n",
      "99.08% completed\n",
      "99.23% completed\n",
      "99.38% completed\n",
      "99.54% completed\n",
      "99.69% completed\n",
      "99.84% completed\n",
      "99.99% completed\n",
      "Completed in 330.40s\n"
     ]
    }
   ],
   "source": [
    "def recall_score(actual, pred):\n",
    "    \"\"\"\n",
    "    Given two lists representing actual and predicted values\n",
    "    Returns the recall of the prediction\n",
    "    \"\"\"\n",
    "    if len(actual) == 0:\n",
    "        return 0\n",
    "    actual, pred = set(actual), set(pred)\n",
    "    return len(actual.intersection(pred)) / len(actual)\n",
    "\n",
    "def new_products(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the list of new products purchased\n",
    "    \"\"\"\n",
    "    actual = row[\"products\"][1:-1]\n",
    "    actual = set([int(p.strip()) for p in actual.strip().split(\",\")])\n",
    "    products_target_user = df_prior_user_products.loc[df_prior_user_products['user_id'] == row[\"user_id\"]].products\n",
    "    liked = set(products_target_user.tolist()[0])\n",
    "    return actual - liked\n",
    "\n",
    "def popular_recommend(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the recall score when popular products are recommended\n",
    "    \"\"\"\n",
    "    actual = new_products(row)\n",
    "    return recall_score(actual, popular_products)\n",
    "\n",
    "def tfidf_recommend(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the recall score when our model recommends products\n",
    "    \"\"\"\n",
    "    actual = row[\"products\"][1:-1]\n",
    "    actual = [int(p.strip()) for p in actual.strip().split(\",\")]\n",
    "    target_user = tf_idf[row[\"user_id\"] - 1]\n",
    "    similarities = cosine_similarity(tf_idf, target_user, False)\n",
    "    cos_vec = similarities.toarray()\n",
    "    productset_target_user, recommended = generateRecommendations(target_user, cos_vec, 20, 10)\n",
    "\n",
    "#     global count, prev_count; prev_count = count; count += 1\n",
    "#     if prev_count / total != count / total:\n",
    "#     print(\"{:.2f}% completed\".format(count / total * 100.0))    \n",
    "    \n",
    "    return recall_score(actual, recommended)\n",
    "\n",
    "def build_eval_df(filepath, df_user_products_test, subset=None):\n",
    "    \"\"\"\n",
    "    Builds a dataframe of recall values of the baseline and our model for all the users\n",
    "    in the test data, and saves its to disk at `filepath`\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Building dataframe with recall values ...\")\n",
    "    \n",
    "    df_eval = df_user_products_test.copy()\n",
    "    if subset:\n",
    "        df_eval = df_eval.sample(n=int(len(df_eval) * subset), random_state=7)\n",
    "    df_eval[\"popular_score\"] = df_eval.apply(popular_recommend, axis=1)\n",
    "    df_eval[\"tfidf_score\"] = df_eval.apply(tfidf_recommend, axis=1)\n",
    "    df_eval.to_csv(filepath) #, index_label=False)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))    \n",
    "\n",
    "\n",
    "# Get the dataframe with recall values of the baseline and the model\n",
    "REBUILD_EVAL_DF = False\n",
    "subset = 0.001\n",
    "\n",
    "# How many users in the test?\n",
    "total = len(df_user_products_test) * subset\n",
    "\n",
    "# Counter\n",
    "count = 0\n",
    "prev_count = -1\n",
    "\n",
    "# Estimated 3 hours to run 20% of the test dataset\n",
    "eval_path = \"../data/eval/eval_tfidf_{}_{}.csv\".format(subset if subset is not None else \"full\", N)\n",
    "if REBUILD_EVAL_DF or not Path(eval_path).exists():\n",
    "    build_eval_df(eval_path, df_user_products_test, subset=subset)\n",
    "df_eval = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0.029163493533928095\n",
      "Baseline: 0.026592372088695602\n"
     ]
    }
   ],
   "source": [
    "# Mean recall scores\n",
    "model_mean_recall, baseline_mean_recall = np.mean(df_eval[\"tfidf_score\"]), np.mean(df_eval[\"popular_score\"])\n",
    "print(\"Model: {:.2f}%\".format(model_mean_recall * 100))\n",
    "print(\"Baseline: {:.2f}%\".format(baseline_mean_recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recommendations through TF-IDF are almost a factor of 8 times better than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

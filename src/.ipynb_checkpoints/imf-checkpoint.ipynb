{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System for Instacart\n",
    "### Matrix Factorization for implicit feedback data using Alternating Least Squares\n",
    "\n",
    "The matrix factorization performed in this notebook is based on the this [paper](http://yifanhu.net/PUB/cf.pdf) by Yehuda Koren et. al which explores an alternative way to represent utility matrices with implicit feedback. We are using the library [`implicit`](https://github.com/benfred/implicit) which implements the outlined algorithm.\n",
    "\n",
    "*Note:* Datafiles are built from scratch in this notebook only if they don't exist on disk. However, to force rebuild any datafile, there will be a `REBUILD_*` constant in the respective cell that should be set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order datasets\n",
    "df_order_products_prior = pd.read_csv(\"../data/order_products__prior.csv\")\n",
    "df_order_products_train = pd.read_csv(\"../data/order_products__train.csv\")\n",
    "df_orders = pd.read_csv(\"../data/orders.csv\") \n",
    "\n",
    "# Products\n",
    "df_products = pd.read_csv(\"../data/products.csv\")\n",
    "# Merge prior orders and products\n",
    "df_merged_order_products_prior = pd.merge(df_order_products_prior, df_products, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data ...\n",
      "Completed in 12.06s\n"
     ]
    }
   ],
   "source": [
    "def make_test_data(filepath, df_orders, df_order_products_train):\n",
    "    \"\"\"\n",
    "    Generates the test dataset and saves it to disk at the given path\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"Creating test data ...\")\n",
    "\n",
    "    # Read train csv\n",
    "    df_order_user_current = df_orders.loc[(df_orders.eval_set == \"train\")].reset_index()\n",
    "    df_order_user_current = df_order_user_current[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Sanity check #1: `current_order_user_df` and `df_order_products_train` should have the same number of \n",
    "    # unique order ids\n",
    "    assert len(df_order_user_current[\"order_id\"].unique()) == len(df_order_products_train[\"order_id\"].unique())\n",
    "\n",
    "    # Convert train dataframe to a similar format\n",
    "    df_order_products_test = df_order_products_train[[\"order_id\", \"product_id\"]]\n",
    "    df_order_products_test = df_order_products_test.groupby(\"order_id\")[\"product_id\"].apply(list).reset_index().rename(columns={\"product_id\": \"products\"})\n",
    "\n",
    "    # Sanity check #2: `df_order_products_test` and `df_order_user_current` should have the same number of \n",
    "    # records before attempting to merge them\n",
    "    assert df_order_products_test.size == df_order_user_current.size\n",
    "\n",
    "    # Merge on order id\n",
    "    df_user_products_test = pd.merge(df_order_user_current, df_order_products_test, on=\"order_id\")\n",
    "    df_user_products_test = df_user_products_test[[\"user_id\", \"products\"]]\n",
    "\n",
    "    # Write to disk\n",
    "    df_user_products_test.to_csv(filepath, index_label=False)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Generate test data if it doesn't exist already\n",
    "REBUILD_TEST_DATA = False\n",
    "test_data_path = \"../data/user_products__test.csv\"\n",
    "if REBUILD_TEST_DATA or not Path(test_data_path).is_file():\n",
    "    make_test_data(test_data_path, df_orders, df_order_products_train)\n",
    "\n",
    "df_user_products_test = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure that the test data isn't corrupted\n",
    "assert len(df_user_products_test) == 131209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating prior user-product data frame ...\n",
      "Completed in 61.38s\n"
     ]
    }
   ],
   "source": [
    "def get_user_product_prior_df(filepath, df_orders, df_order_products_prior):\n",
    "    \"\"\"\n",
    "    Generates a dataframe of users and their prior products purchases, and writes it to disk at the given path\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"Creating prior user-product data frame ...\")\n",
    "    \n",
    "    # Consider ony \"prior\" orders and remove all columns except `user_id` from `df_orders`\n",
    "    df_order_user_prior = df_orders.loc[df_orders.eval_set == \"prior\"]\n",
    "    df_order_user_prior = df_order_user_prior[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Remove all columns except order_id and user_id from df_orders and \n",
    "    # merge the above on `order_id` and remove `order_id`\n",
    "    df_merged = pd.merge(df_order_user_prior, df_order_products_prior[[\"order_id\", \"product_id\"]], on=\"order_id\")\n",
    "    df_user_product_prior = df_merged[[\"user_id\", \"product_id\"]]\n",
    "    df_user_product_prior = df_user_product_prior.groupby([\"user_id\", \"product_id\"]).size().reset_index().rename(columns={0:\"quantity\"})\n",
    "    \n",
    "    # Write to disk\n",
    "    df_user_product_prior.to_csv(filepath, index_label=False)\n",
    "\n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Build dataframe of users and their prior product purchases.\n",
    "# This is needed for building the utility matrix\n",
    "REBUILD_MATRIX_DF = True\n",
    "matrix_df_path = \"../data/user_products__prior.csv\"\n",
    "if REBUILD_MATRIX_DF or not Path(matrix_df_path).is_file():\n",
    "    get_user_product_prior_df(matrix_df_path, df_orders, df_order_products_prior)\n",
    "\n",
    "df_user_product_prior = pd.read_csv(matrix_df_path)\n",
    "df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating product user matrix ...\n",
      "Completed in 11.25s\n"
     ]
    }
   ],
   "source": [
    "def build_product_user_matrix(matrix_path, df_user_product_prior):\n",
    "    \"\"\"\n",
    "    Generates a utility matrix representing purchase history of users, and writes it to disk.\n",
    "    Rows and Columns represent products and users respectively.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Creating product user matrix ...\")\n",
    "\n",
    "    product_user_matrix = sparse.coo_matrix((df_user_product_prior[\"quantity\"],\n",
    "                                            (df_user_product_prior[\"product_id\"].cat.codes.copy(),\n",
    "                                             df_user_product_prior[\"user_id\"].cat.codes.copy())))    \n",
    "    sparse.save_npz(matrix_path, product_user_matrix)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Get the `product x user` utility matrix\n",
    "REBUILD_MATRIX = False\n",
    "matrix_path = \"../data/product_user_matrix.npz\"\n",
    "if REBUILD_MATRIX or not Path(matrix_path).is_file():\n",
    "    build_product_user_matrix(matrix_path, df_user_product_prior)\n",
    "\n",
    "product_user_matrix = sparse.load_npz(matrix_path).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure that the the generated matrix is accurates\n",
    "# User=1 bought product=196 10 times\n",
    "assert product_user_matrix[195, 0] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.8700882953749"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How sparse is the utility matrix?\n",
    "def sparsity(matrix):\n",
    "    \"\"\"\n",
    "    Given a matrix, returns its sparsity\n",
    "    \"\"\"\n",
    "    total_size = matrix.shape[0] * matrix.shape[1]\n",
    "    actual_size = matrix.size\n",
    "    sparsity = (1 - (actual_size / total_size)) * 100\n",
    "    return(sparsity)\n",
    "\n",
    "sparsity(product_user_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit Matrix Factorization using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building IMF model with alpha: 15\n",
      "Completed in 122.95s\n"
     ]
    }
   ],
   "source": [
    "def confidence_matrix(prod_user_matrix, alpha):\n",
    "    \"\"\"\n",
    "    Given a utility matrix,\n",
    "    Returns the given matrix converted to a confidence matrix\n",
    "    For more details, look at http://yifanhu.net/PUB/cf.pdf\n",
    "    \"\"\"\n",
    "    return (prod_user_matrix * alpha).astype(\"double\")\n",
    "    \n",
    "\n",
    "def build_imf(prod_user_matrix, **kwargs):\n",
    "    \"\"\"\n",
    "    Given the utility matrix and model parameters,\n",
    "    Builds models and writes it to disk at \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building IMF model with alpha: {} ...\".format(kwargs[\"alpha\"]))\n",
    "    model = AlternatingLeastSquares()\n",
    "    model.approximate_similar_items = False\n",
    "    \n",
    "    model.fit(confidence_matrix(prod_user_matrix, kwargs[\"alpha\"]))\n",
    "\n",
    "    # Save model to disk\n",
    "    with open(kwargs[\"path\"], \"wb+\") as f:\n",
    "        pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "    \n",
    "# Specify model params and build it\n",
    "## Alpha's in the range [10, 50] with a step size of 5 were tried. alpha = 15 was found to have the best overall \n",
    "## recall value. \n",
    "model_params = {\"alpha\": 15} \n",
    "model_params[\"path\"] = \"../models/imf/{}.imf\".format(model_params[\"alpha\"])\n",
    "\n",
    "REBUILD_MODEL = False\n",
    "if REBUILD_MODEL or not Path(model_params[\"path\"]).exists():\n",
    "    build_imf(product_user_matrix, **model_params)\n",
    "with open(model_params[\"path\"], \"rb\") as f:\n",
    "    imf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the utility matrix is 0-indexed, the below dict is required to convert between `ids` and `indices`.\n",
    "# For example, `product_id` 1 in the dataset is represented by the `0`th row of the utility matrix.\n",
    "\n",
    "# Maps user_id: user index\n",
    "u_dict = {uid:i for i, uid in enumerate(df_user_product_prior[\"user_id\"].cat.categories)}\n",
    "\n",
    "# Maps product_index: product id\n",
    "p_dict = dict(enumerate(df_user_product_prior[\"product_id\"].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend items for a user 17\n",
    "user_id = 17\n",
    "recommendations = imf_model.recommend(u_dict[user_id], product_user_matrix.T.tocsr(), N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual products bought by user 17\n",
      "['Grade A Extra Large Eggs', 'Select-A-Size Paper Towels, White, 2 Huge Rolls = 5 Regular Rolls  Towels/Napkins', 'Light Spread Butter Substitute', 'Strawberries', 'Raspberries', 'Ultra Soft Bathroom Tissue Double Rolls']\n",
      "\n",
      "Recommendations for user 17\n",
      "['Spaghetti with Meat Sauce Pasta Meal', 'Tomato Ketchup', 'Water', 'Cantaloupe', 'Creamy Peanut Butter', 'Distilled Water', 'Swedish Meatballs', 'Strawberry Ice Cream', 'Natural Sharp Cheddar Sliced Cheese', 'Chocolate Ice Cream']\n"
     ]
    }
   ],
   "source": [
    "# Actual \n",
    "row = df_user_products_test.loc[df_user_products_test.user_id == user_id]\n",
    "actual = list(row[\"products\"])\n",
    "actual = actual[0][1:-1]\n",
    "actual = list(np.array([p.strip() for p in actual.strip().split(\",\")]).astype(np.int64))\n",
    "act_products = []\n",
    "for pid in actual:\n",
    "    act_products.extend((df_products.loc[df_products.product_id == pid].product_name).tolist())\n",
    "print(\"Actual products bought by user {}\\n{}\".format(user_id, act_products))\n",
    "\n",
    "# Recommended\n",
    "r = [p_dict[r[0]] for r in recommendations] # Takes the product_cat_code and maps to product_id\n",
    "rec_products = []\n",
    "for pid in r:\n",
    "    rec_products.extend((df_products.loc[df_products.product_id == pid].product_name).tolist())\n",
    "print(\"\\nRecommendations for user {}\\n{}\".format(user_id, rec_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few similarities in the products purchased by the user and those that are recommended to him. For example, `Select-A-Size White Paper Towels` are a very suitable alternative to `Select-A-Size Paper Towels, White, 2 Huge Rolls = 5 Regular Rolls  Towels/Napkins`.\n",
    "\n",
    "Our recommender is discovery based and hence recommends products that the user has never purchased before. Hence, for evaluation, we are removing products purchased before from his current purchase represented by `actual`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using `Recall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_popular(k, df_merged_order_products_prior):\n",
    "    \"\"\"\n",
    "    Returns the `k` most popular products based on purchase count in the dataset\n",
    "    \"\"\"\n",
    "    popular_products = list(df_merged_order_products_prior[\"product_id\"].value_counts().head(k).index)\n",
    "    return popular_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose of the product_user utility matrix\n",
    "user_product_matrix = product_user_matrix.T.tocsr()\n",
    "\n",
    "# Number of recommendations to make for every user\n",
    "N_REC = 10\n",
    "\n",
    "# Get the `N_REC` most popular products\n",
    "popular_products = get_k_popular(N_REC, df_merged_order_products_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataframe with recall values ...\n",
      "Completed in 100.41s\n"
     ]
    }
   ],
   "source": [
    "def recall_score(actual, pred):\n",
    "    \"\"\"\n",
    "    Given two lists representing actual and predicted values\n",
    "    Returns the recall of the prediction\n",
    "    \"\"\"\n",
    "    if len(actual) == 0:\n",
    "        return 0\n",
    "    actual, pred = set(actual), set(pred)\n",
    "    return len(actual.intersection(pred)) / len(actual)\n",
    "\n",
    "\n",
    "def new_products(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the list of new products purchased\n",
    "    \"\"\"\n",
    "    actual = row[\"products\"][1:-1]  # Products purchased currently \n",
    "    actual = set([int(p.strip()) for p in actual.strip().split(\",\")])\n",
    "    liked = set([p_dict[i] for i in user_product_matrix[u_dict[row[\"user_id\"]]].indices])  # User's purchase history\n",
    "    return actual - liked  # Return only new products purchased\n",
    "\n",
    "\n",
    "def popular_recommend(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the recall score when popular products are recommended\n",
    "    \"\"\"\n",
    "    actual = new_products(row)\n",
    "    return recall_score(actual, popular_products)\n",
    "\n",
    "             \n",
    "def imf_recommend(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the recall score when our model recommends products\n",
    "    \"\"\"\n",
    "    actual = new_products(row)\n",
    "    recommended = imf_model.recommend(u_dict[row[\"user_id\"]], user_product_matrix, N=N_REC)\n",
    "    recommended = [p_dict[r[0]] for r in recommended]\n",
    "    return recall_score(actual, recommended)\n",
    "\n",
    "             \n",
    "def build_eval_df(df_user_products_test, filepath=None, subset=None):\n",
    "    \"\"\"\n",
    "    Builds a dataframe of recall values of the baseline and our model for all the users\n",
    "    in the test data, and saves its to disk at `filepath`\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Building dataframe with recall values ...\")\n",
    "    \n",
    "    df_eval = df_user_products_test.copy()\n",
    "    if subset:\n",
    "        df_eval = df_eval.sample(n=int(len(df_eval) * subset), random_state=7)\n",
    "    df_eval[\"popular_score\"] = df_eval.apply(popular_recommend, axis=1)\n",
    "    df_eval[\"imf_score\"] = df_eval.apply(imf_recommend, axis=1)\n",
    "    \n",
    "    df_eval.to_csv(filepath)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))    \n",
    "\n",
    "\n",
    "# Get the dataframe with recall values of the baseline and the model\n",
    "REBUILD_EVAL_DF = True\n",
    "subset = 0.2  # Evaluate on `subset x 100`% of the test dataset\n",
    "eval_path = \"../data/eval/eval_discovery_{}_{}.csv\".format(subset if subset is not None else \"full\", N_REC)\n",
    "if REBUILD_EVAL_DF or not Path(eval_path).exists():\n",
    "    build_eval_df(df_user_products_test, filepath=eval_path, subset=subset)\n",
    "df_eval = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 4.13%\n",
      "Baseline: 2.62%\n"
     ]
    }
   ],
   "source": [
    "# Mean recall scores\n",
    "model_mean_recall, baseline_mean_recall = np.mean(df_eval[\"imf_score\"]), np.mean(df_eval[\"popular_score\"])\n",
    "print(\"Model: {:.2f}%\".format(model_mean_recall * 100))\n",
    "print(\"Baseline: {:.2f}%\".format(baseline_mean_recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations through matrix factorization are almost a factor of 2 times better than the baseline model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

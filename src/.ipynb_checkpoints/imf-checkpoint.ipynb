{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def sparsity(matrix):\n",
    "    total_size = matrix.shape[0] * matrix.shape[1]\n",
    "    actual_size = matrix.size\n",
    "    sparsity = (1 - (actual_size / total_size)) * 100\n",
    "    return(sparsity)\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    now = datetime.now()\n",
    "    return \"{}_{}_{}_{}{}\".format(now.year, now.month, now.day, now.hour, now.minute)\n",
    "\n",
    "\n",
    "def get_k_popular(k, df_merged_order_products_prior):\n",
    "    popular_products = list(df_merged_order_products_prior[\"product_id\"].value_counts().head(k).index)\n",
    "    return popular_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order datasets\n",
    "df_order_products_prior = pd.read_csv(\"../data/order_products__prior.csv\")\n",
    "df_order_products_train = pd.read_csv(\"../data/order_products__train.csv\")\n",
    "df_orders = pd.read_csv(\"../data/orders.csv\") \n",
    "\n",
    "# Products\n",
    "df_products = pd.read_csv(\"../data/products.csv\")\n",
    "# Merge prior orders and products\n",
    "df_merged_order_products_prior = pd.merge(df_order_products_prior, df_products, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data ...\n",
      "Completed in 13.63s\n"
     ]
    }
   ],
   "source": [
    "def make_test_data(test_data_path, df_orders, df_order_products_train):\n",
    "    start = time.time()\n",
    "    print(\"Creating test data ...\")\n",
    "\n",
    "    # Read train csv\n",
    "    df_order_user_current = df_orders.loc[(df_orders.eval_set == \"train\")].reset_index()\n",
    "    df_order_user_current = df_order_user_current[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Sanity check #1: `current_order_user_df` and `df_order_products_train` should have the same number of \n",
    "    # unique order ids\n",
    "    assert len(df_order_user_current[\"order_id\"].unique()) == len(df_order_products_train[\"order_id\"].unique())\n",
    "\n",
    "    # Convert train dataframe to a similar format\n",
    "    df_order_products_test = df_order_products_train[[\"order_id\", \"product_id\"]]\n",
    "    df_order_products_test = df_order_products_test.groupby(\"order_id\")[\"product_id\"].apply(list).reset_index().rename(columns={\"product_id\": \"products\"})\n",
    "\n",
    "    # Sanity check #2: `df_order_products_test` and `df_order_user_current` should have the same number of \n",
    "    # records before attempting to merge them\n",
    "    assert df_order_products_test.size == df_order_user_current.size\n",
    "\n",
    "    # Merge on order id\n",
    "    df_user_products_test = pd.merge(df_order_user_current, df_order_products_test, on=\"order_id\")\n",
    "    df_user_products_test = df_user_products_test[[\"user_id\", \"products\"]]\n",
    "\n",
    "    # Write to disk\n",
    "    df_user_products_test.to_csv(test_data_path, index_label=False)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Get test data\n",
    "REBUILD_TEST_DATA = False\n",
    "test_data_path = \"../data/user_products__test.csv\"\n",
    "if REBUILD_TEST_DATA or not Path(test_data_path).is_file():\n",
    "    make_test_data(test_data_path, df_orders, df_order_products_train)\n",
    "df_user_products_test = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure that the test data isn't corrupted\n",
    "assert len(df_user_products_test) == 131209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Product Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating prior user - product data frame ...\n",
      "Completed in 61.91s\n"
     ]
    }
   ],
   "source": [
    "def get_user_product_prior_df(filepath, df_orders, df_order_products_prior):\n",
    "    start = time.time()\n",
    "    print(\"Creating prior user-product data frame ...\")\n",
    "    \n",
    "    # Consider ony \"prior\" orders and remove all columns except `user_id` from `df_orders`\n",
    "    df_order_user_prior = df_orders.loc[df_orders.eval_set == \"prior\"]\n",
    "    df_order_user_prior = df_order_user_prior[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Remove all columns except order_id and user_id from df_orders and \n",
    "    # merge the above on `order_id` and remove `order_id`\n",
    "    df_merged = pd.merge(df_order_user_prior, df_order_products_prior[[\"order_id\", \"product_id\"]], on=\"order_id\")\n",
    "    df_user_product_prior = df_merged[[\"user_id\", \"product_id\"]]\n",
    "    df_user_product_prior = df_user_product_prior.groupby([\"user_id\", \"product_id\"]).size().reset_index().rename(columns={0:\"quantity\"})\n",
    "    \n",
    "    # Write to disk\n",
    "    df_user_product_prior.to_csv(filepath, index_label=False)\n",
    "\n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Build dataframe of users, products and quantity bought using prior datasets\n",
    "REBUILD_MATRIX_DF = False\n",
    "matrix_df_path = \"../data/user_products__prior.csv\"\n",
    "if REBUILD_MATRIX_DF or not Path(matrix_df_path).is_file():\n",
    "    get_user_product_prior_df(matrix_df_path, df_orders, df_order_products_prior)\n",
    "df_user_product_prior = pd.read_csv(matrix_df_path)\n",
    "df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating product user matrix ...\n",
      "Completed in 11.04s\n"
     ]
    }
   ],
   "source": [
    "def build_product_user_matrix(matrix_path, df_user_product_prior):\n",
    "    start = time.time()\n",
    "    print(\"Creating product user matrix ...\")\n",
    "    \n",
    "    # Make the dataframe a sparse matrix\n",
    "    df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "    df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")\n",
    "    product_user_matrix = sparse.coo_matrix((df_user_product_prior[\"quantity\"],\n",
    "                                            (df_user_product_prior[\"product_id\"].cat.codes.copy(),\n",
    "                                             df_user_product_prior[\"user_id\"].cat.codes.copy())))\n",
    "    \n",
    "    sparse.save_npz(matrix_path, product_user_matrix)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Get the `product x user` matrix\n",
    "REBUILD_MATRIX = True\n",
    "matrix_path = \"../data/product_user_matrix.npz\"\n",
    "if REBUILD_MATRIX or not Path(matrix_path).is_file():\n",
    "    build_product_user_matrix(matrix_path, df_user_product_prior)\n",
    "product_user_matrix = sparse.load_npz(matrix_path).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User=1 bought product=196 10 times\n",
    "assert product_user_matrix[195, 0] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.8700882953749"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity(product_user_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TFIDF model ...\n",
      "Completed in 11.11s\n"
     ]
    }
   ],
   "source": [
    "from implicit.nearest_neighbours import TFIDFRecommender\n",
    "\n",
    "\n",
    "def build_tfidf(prod_user_matrix):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building TFIDF model ...\")\n",
    "    model = TFIDFRecommender()\n",
    "    model.fit(prod_user_matrix)\n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "    return model\n",
    "\n",
    "tfidf_model = build_tfidf(product_user_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 64.87s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    u_p_m = product_user_matrix.T.tocsr()\n",
    "    target_user = u_p_m[i]\n",
    "    recommendations = target_user.dot(tfidf_model.similarity)\n",
    "\n",
    "print(\"Completed in {:.2f}s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building IMF model with alpha: 15\n",
      "Completed in 139.15s\n"
     ]
    }
   ],
   "source": [
    "def build_imf(prod_user_matrix, **kwargs):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building IMF model with alpha: {}\".format(kwargs[\"alpha\"]))\n",
    "    model = AlternatingLeastSquares()\n",
    "    model.approximate_similar_items = False\n",
    "    \n",
    "    if isinstance(kwargs[\"alpha\"], int):\n",
    "        model.fit((prod_user_matrix * kwargs[\"alpha\"]).astype(\"double\"))\n",
    "    else:\n",
    "        model.fit(bm25_weight(prod_user_matrix))\n",
    "\n",
    "    # Save model to disk\n",
    "    with open(kwargs[\"path\"], \"wb+\") as f:\n",
    "        pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "# Specify model params and build it\n",
    "model_params = {\"alpha\": 15}\n",
    "model_params[\"path\"] = \"../models/imf/{}.imf\".format(model_params[\"alpha\"])\n",
    "\n",
    "REBUILD_MODEL = False\n",
    "if REBUILD_MODEL or not Path(model_params[\"path\"]).exists():\n",
    "    build_imf(product_user_matrix, **model_params)\n",
    "with open(model_params[\"path\"], \"rb\") as f:\n",
    "    imf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps user_id: user_cat_code\n",
    "u_dict = {uid:i for i, uid in enumerate(df_user_product_prior[\"user_id\"].cat.categories)}\n",
    "\n",
    "# Maps product_cat_code: product_id\n",
    "p_dict = dict(enumerate(df_user_product_prior[\"product_id\"].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend items for a user\n",
    "user_id = 1\n",
    "recommendations = imf_model.recommend(u_dict[user_id], product_user_matrix.T.tocsr(), N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual products bought by user 1\n",
      "['Soda', 'Organic String Cheese', '0% Greek Strained Yogurt', 'XL Pick-A-Size Paper Towel Rolls', 'Milk Chocolate Almonds', 'Pistachios', 'Cinnamon Toast Crunch', 'Aged White Cheddar Popcorn', 'Organic Whole Milk', 'Organic Half & Half', 'Zero Calorie Cola']\n",
      "\n",
      "Recommendations for user 1\n",
      "['Clementines', 'Trail Mix', 'Extra Fancy Unsalted Mixed Nuts', \"Crunchy Oats 'n Honey Granola Bars\", 'Whole Grain Cheddar Baked Snack Crackers', 'Mineral Water', 'Organic Simply Naked Pita Chips', 'Apples', 'Popcorn', 'Drinking Water']\n"
     ]
    }
   ],
   "source": [
    "# Actual \n",
    "row = df_user_products_test.loc[df_user_products_test.user_id == user_id]\n",
    "actual = list(row[\"products\"])\n",
    "actual = actual[0][1:-1]\n",
    "actual = list(np.array([p.strip() for p in actual.strip().split(\",\")]).astype(np.int64))\n",
    "act_products = []\n",
    "for pid in actual:\n",
    "    act_products.extend((df_products.loc[df_products.product_id == pid].product_name).tolist())\n",
    "print(\"Actual products bought by user {}\\n{}\".format(user_id, act_products))\n",
    "\n",
    "# Recommended\n",
    "r = [p_dict[r[0]] for r in recommendations] # Takes the product_cat_code and maps to product_id\n",
    "rec_products = []\n",
    "for pid in r:\n",
    "    rec_products.extend((df_products.loc[df_products.product_id == pid].product_name).tolist())\n",
    "print(\"\\nRecommendations for user {}\\n{}\".format(user_id, rec_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10 most popular products\n",
    "popular_products = get_k_popular(10, df_merged_order_products_prior)\n",
    "\n",
    "# Maps user_id: user_cat_code\n",
    "u_dict = {uid:i for i, uid in enumerate(df_user_product_prior[\"user_id\"].cat.categories)}\n",
    "\n",
    "# Maps product_cat_code: product_id\n",
    "p_dict = dict(enumerate(df_user_product_prior[\"product_id\"].cat.categories))\n",
    "\n",
    "# Transpose of the product_user_matrix\n",
    "user_product_matrix = product_user_matrix.T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 195)\t10\n",
      "  (0, 10254)\t9\n",
      "  (0, 10322)\t1\n",
      "  (0, 12423)\t10\n",
      "  (0, 13028)\t3\n",
      "  (0, 13172)\t2\n",
      "  (0, 14080)\t1\n",
      "  (0, 17118)\t1\n",
      "  (0, 25129)\t8\n",
      "  (0, 26083)\t2\n",
      "  (0, 26400)\t2\n",
      "  (0, 30444)\t1\n",
      "  (0, 35945)\t1\n",
      "  (0, 38920)\t1\n",
      "  (0, 39649)\t1\n",
      "  (0, 41779)\t1\n",
      "  (0, 46139)\t3\n",
      "  (0, 49224)\t2\n"
     ]
    }
   ],
   "source": [
    "print(user_product_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  195, 10254, 10322, 12423, 13028, 13172, 14080, 17118, 25129,\n",
       "       26083, 26400, 30444, 35945, 38920, 39649, 41779, 46139, 49224], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_product_matrix[0].tocsr().indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataframe with recall values ...\n",
      "Completed in 23.43s\n"
     ]
    }
   ],
   "source": [
    "def recall_score(actual, pred):\n",
    "    if len(actual) == 0:\n",
    "        return 0\n",
    "    actual, pred = set(actual), set(pred)\n",
    "    return len(actual.intersection(pred)) / len(actual)\n",
    "\n",
    "\n",
    "def new_products(row):\n",
    "    actual = row[\"products\"][1:-1]\n",
    "    actual = set([int(p.strip()) for p in actual.strip().split(\",\")])\n",
    "    liked = set([p_dict[i] for i in user_product_matrix[u_dict[row[\"user_id\"]]].indices])\n",
    "    return actual - liked\n",
    "\n",
    "\n",
    "def popular_recommend(row):\n",
    "    actual = new_products(row)\n",
    "    return recall_score(actual, popular_products)\n",
    "\n",
    "             \n",
    "def imf_recommend(row):\n",
    "    actual = new_products(row)\n",
    "    recommended = imf_model.recommend(u_dict[row[\"user_id\"]], user_product_matrix, N=10)\n",
    "    recommended = [p_dict[r[0]] for r in recommended]\n",
    "    return recall_score(actual, recommended)\n",
    "\n",
    "             \n",
    "def build_eval_df(product_user_matrix, df_user_products_test, filepath=None, subset=None):\n",
    "    start = time.time()\n",
    "    print(\"Building dataframe with recall values ...\")\n",
    "    \n",
    "    df_eval = df_user_products_test.copy()\n",
    "    if subset:\n",
    "        df_eval = df_eval.sample(n=int(len(df_eval) * subset), random_state=7)\n",
    "    df_eval[\"popular_score\"] = df_eval.apply(popular_recommend, axis=1)\n",
    "    df_eval[\"imf_score\"] = df_eval.apply(imf_recommend, axis=1)\n",
    "    \n",
    "    df_eval.to_csv(filepath)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))    \n",
    "\n",
    "\n",
    "# Get the dataframe with recall values of the baseline and the model\n",
    "REBUILD_EVAL_DF = True\n",
    "subset = 0.05\n",
    "eval_path = \"../data/eval/eval_discovery_{}.csv\".format(subset if subset is not None else \"full\")\n",
    "if REBUILD_EVAL_DF or not Path(eval_path).exists():\n",
    "    build_eval_df(product_user_matrix, df_user_products_test, filepath=eval_path, subset=subset)\n",
    "df_eval = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0.0418546512876242\n",
      "Baseline: 0.02702136841701362\n"
     ]
    }
   ],
   "source": [
    "# Mean recall scores\n",
    "model_mean_recall, baseline_mean_recall = np.mean(df_eval[\"imf_score\"]), np.mean(df_eval[\"popular_score\"])\n",
    "print(\"Model: {}\".format(model_mean_recall))\n",
    "print(\"Baseline: {}\".format(baseline_mean_recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

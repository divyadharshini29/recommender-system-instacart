{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System for Instacart\n",
    "### Collaborative filtering using Matrix Factorization for implicit feedback data using simple SVD largest k Singular values.\n",
    "The matrix factorization performed in this notebook Computes the largest k singular values/vectors for a sparse matrix. Based upon the Largest K singular Values we find top K Recommended item. We are using scipy.sparse.linalg library which implements SVD using ARPACK as an eigensolver on A.H * A or A * A.H, depending on which one is more efficient.\n",
    "Note: Datafiles are built from scratch in this notebook only if they don't exist on disk. However, to force rebuild any datafile, there will be a REBUILD_* constant in the respective cell that should be set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as linalg\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from numpy import bincount, log, sqrt\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths for data files\n",
    "base_path=\"../data/\"\n",
    "product_user_matrix_path=base_path+\"product_user_matrix.npz\"\n",
    "order_products_prior_path=base_path+\"../data/order_products__prior.csv\"\n",
    "order_products_train_path=base_path+\"../data/order_products__train.csv\"\n",
    "orders_path=base_path+\"../data/orders.csv\"\n",
    "products_path=base_path+\"../data/products.csv\"\n",
    "test_data_path = base_path+'user_products__test.csv'\n",
    "matrix_df_path = base_path+\"user_products__prior.csv\"\n",
    "matrix_path = base_path+\"product_user_matrix.npz\"\n",
    "product_factor_50_path= base_path+\"product_factor_50.npy\"\n",
    "user_factor_50_path= base_path+\"user_factor_50.npy\"\n",
    "product_factor_100_path= base_path+\"product_factor_100.npy\"\n",
    "user_factor_100_path= base_path+\"user_factor_100.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order & User Datasets\n",
    "df_order_products_prior = pd.read_csv(order_products_prior_path)\n",
    "df_order_products_train = pd.read_csv(order_products_train_path)\n",
    "df_orders = pd.read_csv(orders_path) \n",
    "\n",
    "# Products\n",
    "df_products = pd.read_csv(products_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data(filepath, df_orders, df_order_products_train):\n",
    "    \"\"\"\n",
    "    Generates the test dataset and saves it to disk at the given path\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"Creating test data ...\")\n",
    "\n",
    "    # Read train csv\n",
    "    df_order_user_current = df_orders.loc[(df_orders.eval_set == \"train\")].reset_index()\n",
    "    df_order_user_current = df_order_user_current[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Sanity check #1: `current_order_user_df` and `df_order_products_train` should have the same number of \n",
    "    # unique order ids\n",
    "    assert len(df_order_user_current[\"order_id\"].unique()) == len(df_order_products_train[\"order_id\"].unique())\n",
    "\n",
    "    # Convert train dataframe to a similar format\n",
    "    df_order_products_test = df_order_products_train[[\"order_id\", \"product_id\"]]\n",
    "    df_order_products_test = df_order_products_test.groupby(\"order_id\")[\"product_id\"].apply(list).reset_index().rename(columns={\"product_id\": \"products\"})\n",
    "\n",
    "    # Sanity check #2: `df_order_products_test` and `df_order_user_current` should have the same number of \n",
    "    # records before attempting to merge them\n",
    "    assert df_order_products_test.size == df_order_user_current.size\n",
    "\n",
    "    # Merge on order id\n",
    "    df_user_products_test = pd.merge(df_order_user_current, df_order_products_test, on=\"order_id\")\n",
    "    df_user_products_test = df_user_products_test[[\"user_id\", \"products\"]]\n",
    "\n",
    "    # Write to disk\n",
    "    df_user_products_test.to_csv(filepath, index_label=False)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Generate test data if it doesn't exist already\n",
    "REBUILD_TEST_DATA = False\n",
    "if REBUILD_TEST_DATA or not Path(test_data_path).is_file():\n",
    "    make_test_data(test_data_path, df_orders, df_order_products_train)\n",
    "\n",
    "df_user_products_test = pd.read_csv(test_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure that the test data isn't corrupted\n",
    "assert len(df_user_products_test) == 131209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Dataframe for user-products-quantity for order_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_product_prior_df(filepath, df_orders, df_order_products_prior):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates a dataframe of users and their prior products purchases, and writes it to disk at the given path\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"Creating prior user product data frame ...\")\n",
    "\n",
    "    df_merged = pd.merge(df_orders, df_order_products_prior, on=\"order_id\")\n",
    "    df_user_product_prior = df_merged[[\"user_id\", \"product_id\"]]\n",
    "    df_user_product_prior = df_user_product_prior.groupby([\"user_id\", \"product_id\"]).size().reset_index().rename(columns={0:\"quantity\"})\n",
    "    \n",
    "    # Write to disk\n",
    "    df_user_product_prior.to_csv(filepath, index_label=False)\n",
    "\n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Build dataframe of users, products and quantity bought using prior datasets\n",
    "REBUILD_MATRIX_DF = False\n",
    "if REBUILD_MATRIX_DF or not Path(matrix_df_path).is_file():\n",
    "    get_user_product_prior_df(matrix_df_path, df_orders, df_order_products_prior)\n",
    "df_user_product_prior = pd.read_csv(matrix_df_path)\n",
    "# Making them as category for making dictonary of user and item ids later for easy \n",
    "# mapping from sparse Matrix representation\n",
    "df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_product_user_matrix(matrix_path, df_user_product_prior):\n",
    "    \"\"\"\n",
    "    Generates a utility matrix representing purchase history of users, and writes it to disk.\n",
    "    Rows and Columns represent products and users respectively.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Creating product user matrix ...\")\n",
    "\n",
    "    product_user_matrix = sparse.coo_matrix((df_user_product_prior[\"quantity\"],\n",
    "                                            (df_user_product_prior[\"product_id\"].cat.codes.copy(),\n",
    "                                             df_user_product_prior[\"user_id\"].cat.codes.copy())))    \n",
    "    sparse.save_npz(matrix_path, product_user_matrix)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe of users, products and quantity bought using prior datasets\n",
    "REBUILD_USER_MATRIX_DF = False\n",
    "if REBUILD_USER_MATRIX_DF or not Path(matrix_path).is_file():\n",
    "    build_product_user_matrix(matrix_path, df_user_product_prior)    \n",
    "product_user_matrix=sparse.load_npz(product_user_matrix_path).tocsr().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure that the the generated matrix is accurates\n",
    "# User=1 bought product=196 10 times\n",
    "assert product_user_matrix[195, 0] == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.8700882953749"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# How sparse is the utility matrix?\n",
    "def sparsity(matrix):\n",
    "    total_size = matrix.shape[0] * matrix.shape[1]\n",
    "    actual_size = matrix.size\n",
    "    sparsity = (1 - (actual_size / total_size)) * 100\n",
    "    return(sparsity)\n",
    "\n",
    "sparsity(product_user_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating User and product factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factors= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_factors_50,user_factors_50 here denote 50 latent Factors considered\n",
    "REBUILD_FACTORS= False\n",
    "if REBUILD_FACTORS or not ((Path(product_factor_50_path)).is_file() \n",
    "                           and (Path(user_factor_50_path)).is_file()): \n",
    "    #Calculating the product and user factors\n",
    "    product_factors_50, S, user_factors_50 = linalg.svds(product_user_matrix, 50)\n",
    "    # changing to user* factor format\n",
    "    user_factors_50=user_factors_50.T*S\n",
    "    # saving the user and product factors\n",
    "    np.save(product_factor_50_path, product_factors_50)\n",
    "    np.save(user_factor_50_path, user_factors_50)\n",
    "else:\n",
    "    # Loading the user and product factors \n",
    "    product_factors_50=np.load(product_factor_50_path)\n",
    "    user_factors_50=np.load(user_factor_50_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factors= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_factors_100,user_factors_100 here denotes 100 latent Factors considered\n",
    "REBUILD_FACTORS= False\n",
    "if REBUILD_FACTORS or not ((Path(product_factor_100_path)).is_file() \n",
    "                           and (Path(user_factor_100_path)).is_file()): \n",
    "    #Calculating the product and user factors\n",
    "    product_factors_100, S, user_factors_100 = linalg.svds(product_user_matrix, 100)\n",
    "    # changing to user* factor format\n",
    "    user_factors_100=user_factors_100.T*S\n",
    "    # saving the user and product factors\n",
    "    np.save(product_factor_100_path, product_factors_100)\n",
    "    np.save(user_factor_100_path, user_factors_100)\n",
    "else:\n",
    "    # Loading the user and product factors \n",
    "    product_factors_100=np.load(product_factor_100_path)\n",
    "    user_factors_100=np.load(user_factor_100_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class To find the top recommended items given a user_id\n",
    "class TopRecommended(object):\n",
    "    def __init__(self, product_factors,user_factors,product_user_matrix):\n",
    "        self.product_factors =product_factors\n",
    "        self.user_factors =user_factors\n",
    "        self.product_user_matrix=product_user_matrix\n",
    "    def recommend(self, user_id, N=10):\n",
    "        \n",
    "        \"\"\"\n",
    "        Finds top K Recommendations\n",
    "        \"\"\"\n",
    "        scores =  self.user_factors[user_id].dot(self.product_factors.T)\n",
    "        best = np.argpartition(scores, -N)[-N:]\n",
    "        return sorted(zip(best, scores[best]), key=lambda x: -x[1])\n",
    "\n",
    "    def recommend_new(self, user_id, N=10):\n",
    "        \"\"\"\n",
    "        Finds Top k new Recommendations\n",
    "        \"\"\"\n",
    "        scores =  self.user_factors[user_id].dot(self.product_factors.T)\n",
    "        bought_indices=product_user_matrix.T[user_id].nonzero()[1]\n",
    "        count = N + len(bought_indices)\n",
    "        ids = np.argpartition(scores, -count)[-count:]\n",
    "        best = sorted(zip(ids, scores[ids]), key=lambda x: -x[1])        \n",
    "        return list(itertools.islice((rec for rec in best if rec[0] not in bought_indices), N))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictonary to map User_id & Product_id in Utility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the utility matrix is 0-indexed, the below dict is required to convert between `ids` and `indices`.\n",
    "# For example, `product_id` 1 in the dataset is represented by the `0`th row of the utility matrix.\n",
    "\n",
    "# Maps user_id: user index\n",
    "u_dict = {uid:i for i, uid in enumerate(df_user_product_prior[\"user_id\"].cat.categories)}\n",
    "\n",
    "# Maps product_index: product id\n",
    "p_dict = dict(enumerate(df_user_product_prior[\"product_id\"].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing class for factors 50 which returns top recommended items for a user_id\n",
    "svd_recm=TopRecommended(product_factors_50,user_factors_50,product_user_matrix)\n",
    "\n",
    "# Initializing class for factors 100 which returns top recommended items for a user_id\n",
    "svd_recm_100=TopRecommended(product_factors_100,user_factors_100,product_user_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID : 1\n"
     ]
    }
   ],
   "source": [
    "# Recommend items for a user 1\n",
    "user_id = 1\n",
    "print(\"User ID :\",user_id)\n",
    "# New Recommendations and Old Recommendations\n",
    "recommendations_all = svd_recm.recommend(u_dict[user_id],N=10)\n",
    "recommendations_new = svd_recm.recommend_new(u_dict[user_id],N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Products Bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual products bought by user 1\n",
      "['Soda', 'Organic String Cheese', '0% Greek Strained Yogurt', 'XL Pick-A-Size Paper Towel Rolls', 'Milk Chocolate Almonds', 'Pistachios', 'Cinnamon Toast Crunch', 'Aged White Cheddar Popcorn', 'Organic Whole Milk', 'Organic Half & Half', 'Zero Calorie Cola']\n",
      "\n",
      "\n",
      "All products recommended to user 1\n",
      "['Soda', 'Clementines', '0% Greek Strained Yogurt', 'Bag of Organic Bananas', 'Organic Half & Half', 'Trail Mix', 'Apples', 'Extra Fancy Unsalted Mixed Nuts', 'Zero Calorie Cola', 'Reduced Fat 2% Milk']\n",
      "\n",
      "\n",
      "New products recommended to user 1\n",
      "['Clementines', 'Trail Mix', 'Apples', 'Extra Fancy Unsalted Mixed Nuts', 'Reduced Fat 2% Milk', 'Sparkling Mineral Water', \"Crunchy Oats 'n Honey Granola Bars\", 'Mixed Fruit Fruit Snacks', 'Mozzarella String Cheese', 'Popcorn']\n"
     ]
    }
   ],
   "source": [
    "# Actual\n",
    "row = df_user_products_test.loc[df_user_products_test.user_id == user_id]\n",
    "actual = list(row[\"products\"])\n",
    "\n",
    "actual = actual[0][1:-1]\n",
    "actual = list(np.array([p.strip() for p in actual.strip().split(\",\")]).astype(np.int64))\n",
    "act_products = []\n",
    "for pid in actual:\n",
    "    act_products.extend((df_products.loc[df_products.product_id == pid].product_name).tolist())\n",
    "print(\"Actual products bought by user {}\\n{}\\n\\n\".format(user_id, act_products))\n",
    "\n",
    "# All Products Recommended \n",
    "all_recm_products=[]\n",
    "for recommend in recommendations_all:\n",
    "    all_recm_products.extend((df_products.loc[df_products.product_id == p_dict[recommend[0]]].product_name).tolist())\n",
    "print(\"All products recommended to user {}\\n{}\\n\\n\".format(user_id, all_recm_products))\n",
    "\n",
    "\n",
    "# New Products Recommended \n",
    "new_recm_products=[]\n",
    "for recommend in recommendations_new:\n",
    "    new_recm_products.extend((df_products.loc[df_products.product_id == p_dict[recommend[0]]].product_name).tolist())\n",
    "print(\"New products recommended to user {}\\n{}\".format(user_id, new_recm_products))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def get_k_popular(k, df_order_products_prior):\n",
    "    popular_products = list(df_order_products_prior[\"product_id\"].value_counts().head(k).index)\n",
    "    return popular_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose of the product_user utility matrix\n",
    "user_product_matrix = product_user_matrix.T.tocsr()\n",
    "\n",
    "# Number of recommendations to make for every user\n",
    "N_REC = 10\n",
    "\n",
    "# Get the `N_REC` most popular products\n",
    "popular_products = get_k_popular(N_REC, df_order_products_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(actual, pred):\n",
    "    \"\"\"\n",
    "    Given two lists representing actual and predicted values\n",
    "    Returns the recall of the prediction\n",
    "    \"\"\"\n",
    "    if len(actual) == 0:\n",
    "        return 0\n",
    "    actual, pred = set(actual), set(pred)\n",
    "    return len(actual.intersection(pred)) / len(actual)\n",
    "\n",
    "\n",
    "def new_products(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the list of new products purchased\n",
    "    \"\"\"\n",
    "    actual = row[\"products\"][1:-1]  # Products purchased currently \n",
    "    actual = set([int(p.strip()) for p in actual.strip().split(\",\")])\n",
    "    liked = set([p_dict[i] for i in user_product_matrix[u_dict[row[\"user_id\"]]].indices])  # User's purchase history\n",
    "    return actual - liked  # Return only new products purchased\n",
    "\n",
    "\n",
    "def popular_recommend(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the recall score when popular products are recommended\n",
    "    \"\"\"\n",
    "    actual = new_products(row)\n",
    "    return recall_score(actual, popular_products)\n",
    "\n",
    "             \n",
    "def svd_recommend_50_new(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the recall score when our model recommends new products\n",
    "    \"\"\"    \n",
    "    actual = new_products(row)\n",
    "    recommended = svd_recm.recommend_new(u_dict[row[\"user_id\"]], N=N_REC)\n",
    "    recommended = [p_dict[r[0]] for r in recommended]\n",
    "    return recall_score(actual, recommended)\n",
    "\n",
    "def svd_recommend_100_new(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset\n",
    "    Returns the recall score when our model recommends new products\n",
    "    \"\"\"    \n",
    "    actual = new_products(row)\n",
    "    recommended = svd_recm_100.recommend_new(u_dict[row[\"user_id\"]], N=N_REC)\n",
    "    recommended = [p_dict[r[0]] for r in recommended]\n",
    "    return recall_score(actual, recommended)\n",
    "\n",
    "\n",
    "             \n",
    "def build_eval_df(df_user_products_test, filepath=None, subset=None):\n",
    "    \"\"\"\n",
    "    Builds a dataframe of recall values of the baseline and our model for all the users\n",
    "    in the test data, and saves its to disk at `filepath`\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Building dataframe with recall values ...\")\n",
    "    \n",
    "    df_eval = df_user_products_test.copy()\n",
    "    if subset:\n",
    "        df_eval = df_eval.sample(n=int(len(df_eval) * subset), random_state=7)\n",
    "    df_eval[\"popular_score\"] = df_eval.apply(popular_recommend, axis=1)\n",
    "    df_eval[\"svd_new_score_50\"] = df_eval.apply(svd_recommend_50_new, axis=1)\n",
    "    df_eval[\"svd_new_score_100\"] = df_eval.apply(svd_recommend_100_new, axis=1)\n",
    "    df_eval.to_csv(filepath)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataframe with recall values ...\n",
      "Completed in -15887.73s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the dataframe with recall values of the baseline and the model\n",
    "REBUILD_EVAL_DF = True\n",
    "subset = 0.2  # Evaluate on `subset x 100`% of the test dataset\n",
    "eval_path = \"../data/eval/eval_discovery_svd_{}_{}.csv\".format(subset if subset is not None else \"full\", N_REC)\n",
    "if REBUILD_EVAL_DF or not Path(eval_path).exists():\n",
    "    build_eval_df(df_user_products_test, filepath=eval_path, subset=subset)\n",
    "df_eval = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD 100 Factor Model: 2.39%\n",
      "SVD 50 Factor Model: 2.84%\n",
      "Baseline: 2.62%\n"
     ]
    }
   ],
   "source": [
    "# Mean recall scores\n",
    "model_50_mean_recall,model_100_mean_recall, baseline_mean_recall = \\\n",
    "np.mean(df_eval[\"svd_new_score_50\"]),np.mean(df_eval[\"svd_new_score_100\"]), np.mean(df_eval[\"popular_score\"])\n",
    "print(\"SVD 100 Factor Model: {:.2f}%\".format(model_100_mean_recall * 100))\n",
    "print(\"SVD 50 Factor Model: {:.2f}%\".format(model_50_mean_recall * 100))\n",
    "print(\"Baseline: {:.2f}%\".format(baseline_mean_recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 50 factor SVD performs slightly better than the Baseline Model, but because of the overfitting in t the 100 Factors model their results are lower than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

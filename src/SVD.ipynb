{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as linalg\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from numpy import bincount, log, sqrt\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for data files\n",
    "base_path=\"../data/\"\n",
    "base_path_factors='../factors/'\n",
    "product_user_matrix_path=base_path+\"product_user_matrix.npz\"\n",
    "product_factors_svd_path=base_path_factors+\"product_factors_svd\"\n",
    "user_factors_svd_path=base_path_factors+\"user_factors_svd\"\n",
    "\n",
    "product_factors_svd_path_saved=base_path_factors+\"product_factors_svd.npy\"\n",
    "user_factors_svd_path_saved=base_path_factors+\"user_factors_svd.npy\"\n",
    "\n",
    "test_data_path = base_path+'user_products__test.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order datasets\n",
    "df_order_products_prior = pd.read_csv(\"../data/order_products__prior.csv\")\n",
    "df_order_products_train = pd.read_csv(\"../data/order_products__train.csv\")\n",
    "df_orders = pd.read_csv(\"../data/orders.csv\") \n",
    "\n",
    "# Products\n",
    "df_products = pd.read_csv(\"../data/products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data(test_data_path, df_orders, df_order_products_train):\n",
    "    start = time.time()\n",
    "    print(\"Creating test data ...\")\n",
    "\n",
    "    # Read train csv\n",
    "    df_order_user_current = df_orders.loc[(df_orders.eval_set == \"train\")].reset_index()\n",
    "    df_order_user_current = df_order_user_current[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Sanity check #1: `current_order_user_df` and `df_order_products_train` should have the same number of \n",
    "    # unique order ids\n",
    "    assert len(df_order_user_current[\"order_id\"].unique()) == len(df_order_products_train[\"order_id\"].unique())\n",
    "\n",
    "    # Convert train dataframe to a similar format\n",
    "    df_order_products_test = df_order_products_train[[\"order_id\", \"product_id\"]]\n",
    "    df_order_products_test = df_order_products_test.groupby(\"order_id\")[\"product_id\"].apply(list).reset_index().rename(columns={\"product_id\": \"products\"})\n",
    "\n",
    "    # Sanity check #2: `df_order_products_test` and `df_order_user_current` should have the same number of \n",
    "    # records before attempting to merge them\n",
    "    assert df_order_products_test.size == df_order_user_current.size\n",
    "\n",
    "    # Merge on order id\n",
    "    df_user_products_test = pd.merge(df_order_user_current, df_order_products_test, on=\"order_id\")\n",
    "    df_user_products_test = df_user_products_test[[\"user_id\", \"products\"]]\n",
    "\n",
    "    # Write to disk\n",
    "    df_user_products_test.to_csv(test_data_path, index_label=False)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Get test data\n",
    "REBUILD_TEST_DATA = False\n",
    "test_data_path = \"../data/user_products__test.csv\"\n",
    "if REBUILD_TEST_DATA or not Path(test_data_path).is_file():\n",
    "    make_test_data(test_data_path, df_orders, df_order_products_train)\n",
    "df_user_products_test = pd.read_csv(test_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading user_products__prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_product_prior_df(filepath, df_orders, df_order_products_prior):\n",
    "    start = time.time()\n",
    "    print(\"Creating prior user product data frame ...\")\n",
    "\n",
    "    df_merged = pd.merge(df_orders, df_order_products_prior, on=\"order_id\")\n",
    "    df_user_product_prior = df_merged[[\"user_id\", \"product_id\"]]\n",
    "    df_user_product_prior = df_user_product_prior.groupby([\"user_id\", \"product_id\"]).size().reset_index().rename(columns={0:\"quantity\"})\n",
    "    \n",
    "    # Write to disk\n",
    "    df_user_product_prior.to_csv(filepath, index_label=False)\n",
    "\n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "\n",
    "# Build dataframe of users, products and quantity bought using prior datasets\n",
    "REBUILD_MATRIX_DF = False\n",
    "matrix_df_path = \"../data/user_products__prior.csv\"\n",
    "if REBUILD_MATRIX_DF or not Path(matrix_df_path).is_file():\n",
    "    get_user_product_prior_df(matrix_df_path, df_orders, df_order_products_prior)\n",
    "df_user_product_prior = pd.read_csv(matrix_df_path)\n",
    "df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Dictonary of user_id and Product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps user_id: user_cat_code\n",
    "u_dict = {uid:i for i, uid in enumerate(df_user_product_prior[\"user_id\"].cat.categories)}\n",
    "\n",
    "# Maps product_cat_code: product_id\n",
    "p_dict = dict(enumerate(df_user_product_prior[\"product_id\"].cat.categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading product User Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_product_user_matrix(matrix_path, df_user_product_prior):\n",
    "    start = time.time()\n",
    "    print(\"Creating product user matrix ...\")\n",
    "    \n",
    "    # Make the dataframe a sparse matrix\n",
    "    df_user_product_prior[\"user_id\"] = df_user_product_prior[\"user_id\"].astype(\"category\")\n",
    "    df_user_product_prior[\"product_id\"] = df_user_product_prior[\"product_id\"].astype(\"category\")\n",
    "    product_user_matrix = sparse.coo_matrix((df_user_product_prior[\"quantity\"],\n",
    "                                            (df_user_product_prior[\"product_id\"].cat.codes.copy(),\n",
    "                                             df_user_product_prior[\"user_id\"].cat.codes.copy())))\n",
    "    \n",
    "    sparse.save_npz(matrix_path, product_user_matrix)\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe of users, products and quantity bought using prior datasets\n",
    "REBUILD_USER_MATRIX_DF = False\n",
    "if REBUILD_USER_MATRIX_DF or not Path(product_user_matrix_path).is_file():\n",
    "    build_product_user_matrix(matrix_path, df_user_product_prior)    \n",
    "else:\n",
    "    product_user_matrix=sparse.load_npz(product_user_matrix_path).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM_25 weight of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_weight(X, K1=100, B=0.8):\n",
    "    \"\"\" Weighs each row of a sparse matrix X  by BM25 weighting \"\"\"\n",
    "    # calculate idf per term (user)\n",
    "    X = coo_matrix(X)\n",
    "\n",
    "    N = float(X.shape[0])\n",
    "    idf = log(N / (1 + bincount(X.col)))\n",
    "\n",
    "    # calculate length_norm per document (product)\n",
    "    row_sums = np.ravel(X.sum(axis=1))\n",
    "    average_length = row_sums.mean()\n",
    "    length_norm = (1.0 - B) + B * row_sums / average_length\n",
    "\n",
    "    # weight matrix rows by bm25\n",
    "    X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating User and product factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_FACTORS= False\n",
    "if REBUILD_FACTORS or not ((Path(product_factors_svd_path_saved)).is_file() \n",
    "                           and (Path(user_factors_svd_path_saved)).is_file()): \n",
    "    #Calculating the product and user factors\n",
    "    product_factors, _, user_factors = linalg.svds(bm25_weight(product_user_matrix), 50)\n",
    "    # saving the user and product factors\n",
    "    np.save(product_factors_svd_path, product_factors)\n",
    "    np.save(user_factors_svd_path, user_factors)\n",
    "else:\n",
    "    # Loading the user and product factors \n",
    "    product_factors=np.load(product_factors_svd_path_saved)\n",
    "    user_factors=np.load(user_factors_svd_path_saved)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def sparsity(matrix):\n",
    "    total_size = matrix.shape[0] * matrix.shape[1]\n",
    "    actual_size = matrix.size\n",
    "    sparsity = (1 - (actual_size / total_size)) * 100\n",
    "    return(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.8700882953749"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the sparsity\n",
    "sparsity(product_user_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the top related items\n",
    "class TopRelated(object):\n",
    "    def __init__(self, product_factors):\n",
    "        # fully normalize artist_factors, so can compare with only the dot product\n",
    "        norms = np.linalg.norm(product_factors, axis=-1)\n",
    "        self.factors = product_factors / norms[:, np.newaxis]\n",
    "\n",
    "    def get_related(self, product_id, N=10):\n",
    "        scores = self.factors.dot(self.factors[product_id])\n",
    "        best = np.argpartition(scores, -N)[-N:]\n",
    "        return sorted(zip(best, scores[best]), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the top recommended items\n",
    "class TopRecommended(object):\n",
    "    def __init__(self, product_factors,user_factors):\n",
    "        self.product_factors =product_factors\n",
    "        self.user_factors =user_factors\n",
    "\n",
    "    def get_recommended(self, user_id, N=10):\n",
    "        scores =  self.user_factors.T[user_id].dot(self.product_factors.T)\n",
    "        best = np.argpartition(scores, -N)[-N:]\n",
    "        return sorted(zip(best, scores[best]), key=lambda x: -x[1])\n",
    "    \n",
    "    def recommend_new(self, userid, N=10):        \n",
    "        user = self.user_factors.T[userid]\n",
    "#         calculate the top N items, removing the users own liked items from the results\n",
    "        liked = product_user_matrix.T[userid].indices\n",
    "        scores =  user.dot(self.product_factors.T)\n",
    "        count = N + len(liked)\n",
    "        if count < len(scores):\n",
    "            ids = np.argpartition(scores, -count)[-count:]\n",
    "            best = sorted(zip(ids, scores[ids]), key=lambda x: -x[1])\n",
    "        else:\n",
    "            best = sorted(enumerate(scores), key=lambda x: -x[1])\n",
    "        return list(itertools.islice((rec for rec in best if rec[0] not in liked), N))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing class which returns top recommended items for a user_id\n",
    "tp_recm=TopRecommended(product_factors,user_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17536, 4.8472453265396875e-05),\n",
       " (23507, 3.7501003169646856e-05),\n",
       " (18421, 3.3659225008292046e-05),\n",
       " (5754, 3.2425897178474256e-05),\n",
       " (19314, 3.2371654889907893e-05),\n",
       " (19111, 3.215674286486141e-05),\n",
       " (46219, 2.7867475059536311e-05),\n",
       " (1711, 2.7027152450202514e-05),\n",
       " (46368, 2.6843099919577224e-05),\n",
       " (30652, 2.6331188399805843e-05)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate top new recommended Item for a user\n",
    "tp_recm.recommend_new(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17536, 4.8472453265396875e-05),\n",
       " (23507, 3.7501003169646856e-05),\n",
       " (18421, 3.3659225008292046e-05),\n",
       " (5754, 3.2425897178474256e-05),\n",
       " (19314, 3.2371654889907893e-05),\n",
       " (19111, 3.215674286486141e-05),\n",
       " (46219, 2.7867475059536311e-05),\n",
       " (1711, 2.7027152450202514e-05),\n",
       " (46368, 2.6843099919577224e-05),\n",
       " (30652, 2.6331188399805843e-05)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate top recommended Item for a user\n",
    "tp_recm.get_recommended(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def get_k_popular(k, df_order_products_prior):\n",
    "    popular_products = list(df_order_products_prior[\"product_id\"].value_counts().head(k).index)\n",
    "    return popular_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_products = get_k_popular(10, df_order_products_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model ...\n",
      "Completed in 65.55s\n",
      "Model: 0.007231987137613736\n",
      "Popular: 0.06694641505105668\n"
     ]
    }
   ],
   "source": [
    "# Iterrows approach\n",
    "\n",
    "start = time.time()\n",
    "print(\"Evaluating model ...\")\n",
    "    \n",
    "def recall_score(actual, pred):\n",
    "    actual, pred = set(actual), set(pred)\n",
    "    return len(actual.intersection(pred)) / len(actual)\n",
    "\n",
    "\n",
    "user_product_matrix = product_user_matrix.T \n",
    "model_recalls = []\n",
    "popular_recalls = []\n",
    "\n",
    "for index, row in df_user_products_test.head(1000).iterrows():\n",
    "    actual = row[\"products\"][1:-1]\n",
    "    actual = [int(p.strip()) for p in actual.strip().split(\",\")]\n",
    "    recommended = tp_recm.recommend_new(u_dict[row[\"user_id\"]], N=10)\n",
    "    recommended = [p_dict[r[0]]  for r in recommended]\n",
    "    model_recalls.append(recall_score(actual, recommended))\n",
    "    popular_recalls.append(recall_score(actual, popular_products))\n",
    "\n",
    "    \n",
    "model_mean_recall = np.mean(model_recalls)\n",
    "popular_mean_recall = np.mean(popular_recalls)\n",
    "\n",
    "print(\"Completed in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "print(\"Model: {}\".format(model_mean_recall))\n",
    "print(\"Popular: {}\".format(popular_mean_recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
